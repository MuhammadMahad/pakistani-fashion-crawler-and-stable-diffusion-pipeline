{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08240e72-a10a-4c55-b2c0-038beb996fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'diffusers'...\n",
      "remote: Enumerating objects: 75599, done.\u001b[K\n",
      "remote: Counting objects: 100% (401/401), done.\u001b[K\n",
      "remote: Compressing objects: 100% (249/249), done.\u001b[K\n",
      "remote: Total 75599 (delta 214), reused 256 (delta 124), pack-reused 75198 (from 1)\u001b[K\n",
      "Receiving objects: 100% (75599/75599), 54.29 MiB | 38.90 MiB/s, done.\n",
      "Resolving deltas: 100% (55973/55973), done.\n",
      "/notebooks/diffusers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /notebooks/diffusers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from diffusers==0.32.0.dev0) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers==0.32.0.dev0) (3.13.1)\n",
      "Collecting huggingface-hub>=0.23.2 (from diffusers==0.32.0.dev0)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers==0.32.0.dev0) (1.26.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.32.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers==0.32.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.32.0.dev0) (0.4.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.32.0.dev0) (9.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.32.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers==0.32.0.dev0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.32.0.dev0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers==0.32.0.dev0) (2020.6.20)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.8/447.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: diffusers\n",
      "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for diffusers: filename=diffusers-0.32.0.dev0-py3-none-any.whl size=3000102 sha256=cc282fb9e10fd278c8888f09a34700056f79f2b4eccf4509f370facc52a2dfe6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-s137ru16/wheels/a8/74/df/8502a0e534f7d979b721b2c3985039d30e405a660e118bd9a6\n",
      "Successfully built diffusers\n",
      "Installing collected packages: huggingface-hub, diffusers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.21.4\n",
      "    Uninstalling diffusers-0.21.4:\n",
      "      Successfully uninstalled diffusers-0.21.4\n",
      "Successfully installed diffusers-0.32.0.dev0 huggingface-hub-0.26.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/diffusers\n",
    "%cd diffusers\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14857d1-7e39-44d9-897f-be52107f8dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/diffusers/examples/text_to_image\n",
      "Requirement already satisfied: accelerate>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (0.24.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.16.1+cu121)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.35.2)\n",
      "Collecting datasets>=2.19.1 (from -r requirements.txt (line 4))\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting ftfy (from -r requirements.txt (line 5))\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.15.1)\n",
      "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (3.1.3)\n",
      "Collecting peft==0.7.0 (from -r requirements.txt (line 8))\n",
      "  Downloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 8)) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 8)) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 8)) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from peft==0.7.0->-r requirements.txt (line 8)) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 8)) (2.1.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 8)) (4.66.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 8)) (0.26.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 2)) (9.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (0.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.1->-r requirements.txt (line 4)) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.1->-r requirements.txt (line 4)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.1->-r requirements.txt (line 4)) (2.2.0)\n",
      "Collecting requests (from torchvision->-r requirements.txt (line 2))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from peft==0.7.0->-r requirements.txt (line 8))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.1->-r requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.1->-r requirements.txt (line 4)) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.1->-r requirements.txt (line 4)) (3.9.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->-r requirements.txt (line 5)) (0.2.13)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 6)) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.5.2)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 6)) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 6)) (69.0.3)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2->-r requirements.txt (line 7)) (2.1.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.1->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets>=2.19.1->-r requirements.txt (line 4)) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.1->-r requirements.txt (line 4)) (2023.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/lib/python3/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Downloading peft-0.7.0-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, requests, ftfy, datasets, peft\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.5\n",
      "    Uninstalling datasets-2.14.5:\n",
      "      Successfully uninstalled datasets-2.14.5\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.6.2\n",
      "    Uninstalling peft-0.6.2:\n",
      "      Successfully uninstalled peft-0.6.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 ftfy-6.3.1 peft-0.7.0 requests-2.32.3 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd examples/text_to_image\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e0b219-a62c-489e-9c68-3bf427796c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
     ]
    }
   ],
   "source": [
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edadfbc-7ad1-4868-96bf-7da08246df18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4f6c2-4776-4cfb-96c5-3f7b61338a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 08:56:55.946377: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-08 08:56:55.946495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-08 08:56:55.948187: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-08 08:56:55.957436: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-08 08:56:56.808479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "12/08/2024 08:56:57 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'dynamic_thresholding_ratio', 'thresholding', 'timestep_spacing', 'prediction_type', 'variance_type', 'clip_sample_range', 'rescale_betas_zero_snr', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'shift_factor', 'use_post_quant_conv', 'scaling_factor', 'mid_block_add_attention', 'use_quant_conv', 'latents_std', 'force_upcast', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_type', 'use_linear_projection', 'class_embeddings_concat', 'addition_embed_type', 'transformer_layers_per_block', 'time_embedding_type', 'dropout', 'resnet_time_scale_shift', 'reverse_transformer_layers_per_block', 'class_embed_type', 'cross_attention_norm', 'upcast_attention', 'conv_in_kernel', 'attention_type', 'addition_time_embed_dim', 'dual_cross_attention', 'resnet_out_scale_factor', 'time_embedding_dim', 'time_embedding_act_fn', 'addition_embed_type_num_heads', 'timestep_post_act', 'num_class_embeds', 'encoder_hid_dim_type', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'only_cross_attention', 'resnet_skip_time_act', 'conv_out_kernel', 'projection_class_embeddings_input_dim', 'num_attention_heads'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_type', 'use_linear_projection', 'class_embeddings_concat', 'addition_embed_type', 'transformer_layers_per_block', 'time_embedding_type', 'dropout', 'resnet_time_scale_shift', 'reverse_transformer_layers_per_block', 'class_embed_type', 'cross_attention_norm', 'upcast_attention', 'conv_in_kernel', 'attention_type', 'addition_time_embed_dim', 'dual_cross_attention', 'resnet_out_scale_factor', 'time_embedding_dim', 'time_embedding_act_fn', 'addition_embed_type_num_heads', 'timestep_post_act', 'num_class_embeds', 'encoder_hid_dim_type', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'only_cross_attention', 'resnet_skip_time_act', 'conv_out_kernel', 'projection_class_embeddings_input_dim', 'num_attention_heads'} was not found in config. Values will be initialized to default values.\n",
      "Resolving data files: 100%|███████████████| 9999/9999 [00:01<00:00, 9183.38it/s]\n",
      "12/08/2024 08:57:06 - INFO - __main__ - ***** Running training *****\n",
      "12/08/2024 08:57:06 - INFO - __main__ -   Num examples = 9998\n",
      "12/08/2024 08:57:06 - INFO - __main__ -   Num Epochs = 6\n",
      "12/08/2024 08:57:06 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "12/08/2024 08:57:06 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "12/08/2024 08:57:06 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "12/08/2024 08:57:06 - INFO - __main__ -   Total optimization steps = 15000\n",
      "Steps:   3%|▏   | 500/15000 [08:49<4:17:47,  1.07s/it, lr=1e-5, step_loss=0.188]12/08/2024 09:05:56 - INFO - accelerate.accelerator - Saving current state to pakistani-fashion-generator-model/checkpoint-500\n",
      "{'mid_block_type', 'use_linear_projection', 'class_embeddings_concat', 'addition_embed_type', 'transformer_layers_per_block', 'time_embedding_type', 'dropout', 'resnet_time_scale_shift', 'reverse_transformer_layers_per_block', 'class_embed_type', 'cross_attention_norm', 'upcast_attention', 'conv_in_kernel', 'attention_type', 'addition_time_embed_dim', 'dual_cross_attention', 'resnet_out_scale_factor', 'time_embedding_dim', 'time_embedding_act_fn', 'addition_embed_type_num_heads', 'timestep_post_act', 'num_class_embeds', 'encoder_hid_dim_type', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'only_cross_attention', 'resnet_skip_time_act', 'conv_out_kernel', 'projection_class_embeddings_input_dim', 'num_attention_heads'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-500/unet_ema/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-500/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-500/unet/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-500/unet/diffusion_pytorch_model.safetensors\n",
      "12/08/2024 09:06:17 - INFO - accelerate.checkpointing - Optimizer state saved in pakistani-fashion-generator-model/checkpoint-500/optimizer.bin\n",
      "12/08/2024 09:06:17 - INFO - accelerate.checkpointing - Scheduler state saved in pakistani-fashion-generator-model/checkpoint-500/scheduler.bin\n",
      "12/08/2024 09:06:17 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in pakistani-fashion-generator-model/checkpoint-500/sampler.bin\n",
      "12/08/2024 09:06:17 - INFO - accelerate.checkpointing - Gradient scaler state saved in pakistani-fashion-generator-model/checkpoint-500/scaler.pt\n",
      "12/08/2024 09:06:17 - INFO - accelerate.checkpointing - Random states saved in pakistani-fashion-generator-model/checkpoint-500/random_states_0.pkl\n",
      "12/08/2024 09:06:17 - INFO - __main__ - Saved state to pakistani-fashion-generator-model/checkpoint-500\n",
      "Steps:   7%|▏ | 1000/15000 [17:58<4:15:54,  1.10s/it, lr=1e-5, step_loss=0.0348]12/08/2024 09:15:05 - INFO - accelerate.accelerator - Saving current state to pakistani-fashion-generator-model/checkpoint-1000\n",
      "{'mid_block_type', 'use_linear_projection', 'class_embeddings_concat', 'addition_embed_type', 'transformer_layers_per_block', 'time_embedding_type', 'dropout', 'resnet_time_scale_shift', 'reverse_transformer_layers_per_block', 'class_embed_type', 'cross_attention_norm', 'upcast_attention', 'conv_in_kernel', 'attention_type', 'addition_time_embed_dim', 'dual_cross_attention', 'resnet_out_scale_factor', 'time_embedding_dim', 'time_embedding_act_fn', 'addition_embed_type_num_heads', 'timestep_post_act', 'num_class_embeds', 'encoder_hid_dim_type', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'only_cross_attention', 'resnet_skip_time_act', 'conv_out_kernel', 'projection_class_embeddings_input_dim', 'num_attention_heads'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-1000/unet_ema/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-1000/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-1000/unet/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-1000/unet/diffusion_pytorch_model.safetensors\n",
      "12/08/2024 09:15:27 - INFO - accelerate.checkpointing - Optimizer state saved in pakistani-fashion-generator-model/checkpoint-1000/optimizer.bin\n",
      "12/08/2024 09:15:27 - INFO - accelerate.checkpointing - Scheduler state saved in pakistani-fashion-generator-model/checkpoint-1000/scheduler.bin\n",
      "12/08/2024 09:15:27 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in pakistani-fashion-generator-model/checkpoint-1000/sampler.bin\n",
      "12/08/2024 09:15:27 - INFO - accelerate.checkpointing - Gradient scaler state saved in pakistani-fashion-generator-model/checkpoint-1000/scaler.pt\n",
      "12/08/2024 09:15:27 - INFO - accelerate.checkpointing - Random states saved in pakistani-fashion-generator-model/checkpoint-1000/random_states_0.pkl\n",
      "12/08/2024 09:15:27 - INFO - __main__ - Saved state to pakistani-fashion-generator-model/checkpoint-1000\n",
      "Steps:  10%|▎  | 1500/15000 [27:20<4:23:43,  1.17s/it, lr=1e-5, step_loss=0.108]12/08/2024 09:24:27 - INFO - accelerate.accelerator - Saving current state to pakistani-fashion-generator-model/checkpoint-1500\n",
      "{'mid_block_type', 'use_linear_projection', 'class_embeddings_concat', 'addition_embed_type', 'transformer_layers_per_block', 'time_embedding_type', 'dropout', 'resnet_time_scale_shift', 'reverse_transformer_layers_per_block', 'class_embed_type', 'cross_attention_norm', 'upcast_attention', 'conv_in_kernel', 'attention_type', 'addition_time_embed_dim', 'dual_cross_attention', 'resnet_out_scale_factor', 'time_embedding_dim', 'time_embedding_act_fn', 'addition_embed_type_num_heads', 'timestep_post_act', 'num_class_embeds', 'encoder_hid_dim_type', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'only_cross_attention', 'resnet_skip_time_act', 'conv_out_kernel', 'projection_class_embeddings_input_dim', 'num_attention_heads'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-1500/unet_ema/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-1500/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-1500/unet/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-1500/unet/diffusion_pytorch_model.safetensors\n",
      "12/08/2024 09:24:51 - INFO - accelerate.checkpointing - Optimizer state saved in pakistani-fashion-generator-model/checkpoint-1500/optimizer.bin\n",
      "12/08/2024 09:24:51 - INFO - accelerate.checkpointing - Scheduler state saved in pakistani-fashion-generator-model/checkpoint-1500/scheduler.bin\n",
      "12/08/2024 09:24:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in pakistani-fashion-generator-model/checkpoint-1500/sampler.bin\n",
      "12/08/2024 09:24:51 - INFO - accelerate.checkpointing - Gradient scaler state saved in pakistani-fashion-generator-model/checkpoint-1500/scaler.pt\n",
      "12/08/2024 09:24:51 - INFO - accelerate.checkpointing - Random states saved in pakistani-fashion-generator-model/checkpoint-1500/random_states_0.pkl\n",
      "12/08/2024 09:24:51 - INFO - __main__ - Saved state to pakistani-fashion-generator-model/checkpoint-1500\n",
      "Steps:  13%|▎ | 2000/15000 [36:36<3:54:11,  1.08s/it, lr=1e-5, step_loss=0.0159]12/08/2024 09:33:43 - INFO - accelerate.accelerator - Saving current state to pakistani-fashion-generator-model/checkpoint-2000\n",
      "{'mid_block_type', 'use_linear_projection', 'class_embeddings_concat', 'addition_embed_type', 'transformer_layers_per_block', 'time_embedding_type', 'dropout', 'resnet_time_scale_shift', 'reverse_transformer_layers_per_block', 'class_embed_type', 'cross_attention_norm', 'upcast_attention', 'conv_in_kernel', 'attention_type', 'addition_time_embed_dim', 'dual_cross_attention', 'resnet_out_scale_factor', 'time_embedding_dim', 'time_embedding_act_fn', 'addition_embed_type_num_heads', 'timestep_post_act', 'num_class_embeds', 'encoder_hid_dim_type', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'only_cross_attention', 'resnet_skip_time_act', 'conv_out_kernel', 'projection_class_embeddings_input_dim', 'num_attention_heads'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-2000/unet_ema/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-2000/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-2000/unet/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-2000/unet/diffusion_pytorch_model.safetensors\n",
      "12/08/2024 09:34:04 - INFO - accelerate.checkpointing - Optimizer state saved in pakistani-fashion-generator-model/checkpoint-2000/optimizer.bin\n",
      "12/08/2024 09:34:04 - INFO - accelerate.checkpointing - Scheduler state saved in pakistani-fashion-generator-model/checkpoint-2000/scheduler.bin\n",
      "12/08/2024 09:34:04 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in pakistani-fashion-generator-model/checkpoint-2000/sampler.bin\n",
      "12/08/2024 09:34:04 - INFO - accelerate.checkpointing - Gradient scaler state saved in pakistani-fashion-generator-model/checkpoint-2000/scaler.pt\n",
      "12/08/2024 09:34:04 - INFO - accelerate.checkpointing - Random states saved in pakistani-fashion-generator-model/checkpoint-2000/random_states_0.pkl\n",
      "12/08/2024 09:34:04 - INFO - __main__ - Saved state to pakistani-fashion-generator-model/checkpoint-2000\n",
      "Steps:  17%|▋   | 2500/15000 [46:09<3:10:42,  1.09it/s, lr=1e-5, step_loss=0.14]12/08/2024 09:43:16 - INFO - accelerate.accelerator - Saving current state to pakistani-fashion-generator-model/checkpoint-2500\n",
      "{'mid_block_type', 'use_linear_projection', 'class_embeddings_concat', 'addition_embed_type', 'transformer_layers_per_block', 'time_embedding_type', 'dropout', 'resnet_time_scale_shift', 'reverse_transformer_layers_per_block', 'class_embed_type', 'cross_attention_norm', 'upcast_attention', 'conv_in_kernel', 'attention_type', 'addition_time_embed_dim', 'dual_cross_attention', 'resnet_out_scale_factor', 'time_embedding_dim', 'time_embedding_act_fn', 'addition_embed_type_num_heads', 'timestep_post_act', 'num_class_embeds', 'encoder_hid_dim_type', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'only_cross_attention', 'resnet_skip_time_act', 'conv_out_kernel', 'projection_class_embeddings_input_dim', 'num_attention_heads'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-2500/unet_ema/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-2500/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-2500/unet/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-2500/unet/diffusion_pytorch_model.safetensors\n",
      "12/08/2024 09:43:37 - INFO - accelerate.checkpointing - Optimizer state saved in pakistani-fashion-generator-model/checkpoint-2500/optimizer.bin\n",
      "12/08/2024 09:43:37 - INFO - accelerate.checkpointing - Scheduler state saved in pakistani-fashion-generator-model/checkpoint-2500/scheduler.bin\n",
      "12/08/2024 09:43:37 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in pakistani-fashion-generator-model/checkpoint-2500/sampler.bin\n",
      "12/08/2024 09:43:37 - INFO - accelerate.checkpointing - Gradient scaler state saved in pakistani-fashion-generator-model/checkpoint-2500/scaler.pt\n",
      "12/08/2024 09:43:37 - INFO - accelerate.checkpointing - Random states saved in pakistani-fashion-generator-model/checkpoint-2500/random_states_0.pkl\n",
      "12/08/2024 09:43:37 - INFO - __main__ - Saved state to pakistani-fashion-generator-model/checkpoint-2500\n",
      "Steps:  20%|▌  | 3000/15000 [55:21<3:29:25,  1.05s/it, lr=1e-5, step_loss=0.158]12/08/2024 09:52:28 - INFO - accelerate.accelerator - Saving current state to pakistani-fashion-generator-model/checkpoint-3000\n",
      "{'mid_block_type', 'use_linear_projection', 'class_embeddings_concat', 'addition_embed_type', 'transformer_layers_per_block', 'time_embedding_type', 'dropout', 'resnet_time_scale_shift', 'reverse_transformer_layers_per_block', 'class_embed_type', 'cross_attention_norm', 'upcast_attention', 'conv_in_kernel', 'attention_type', 'addition_time_embed_dim', 'dual_cross_attention', 'resnet_out_scale_factor', 'time_embedding_dim', 'time_embedding_act_fn', 'addition_embed_type_num_heads', 'timestep_post_act', 'num_class_embeds', 'encoder_hid_dim_type', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'only_cross_attention', 'resnet_skip_time_act', 'conv_out_kernel', 'projection_class_embeddings_input_dim', 'num_attention_heads'} was not found in config. Values will be initialized to default values.\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-3000/unet_ema/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-3000/unet_ema/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in pakistani-fashion-generator-model/checkpoint-3000/unet/config.json\n",
      "Model weights saved in pakistani-fashion-generator-model/checkpoint-3000/unet/diffusion_pytorch_model.safetensors\n",
      "12/08/2024 09:52:49 - INFO - accelerate.checkpointing - Optimizer state saved in pakistani-fashion-generator-model/checkpoint-3000/optimizer.bin\n",
      "12/08/2024 09:52:49 - INFO - accelerate.checkpointing - Scheduler state saved in pakistani-fashion-generator-model/checkpoint-3000/scheduler.bin\n",
      "12/08/2024 09:52:49 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in pakistani-fashion-generator-model/checkpoint-3000/sampler.bin\n",
      "12/08/2024 09:52:49 - INFO - accelerate.checkpointing - Gradient scaler state saved in pakistani-fashion-generator-model/checkpoint-3000/scaler.pt\n",
      "12/08/2024 09:52:49 - INFO - accelerate.checkpointing - Random states saved in pakistani-fashion-generator-model/checkpoint-3000/random_states_0.pkl\n",
      "12/08/2024 09:52:49 - INFO - __main__ - Saved state to pakistani-fashion-generator-model/checkpoint-3000\n",
      "Steps:  23%|▏| 3463/15000 [1:03:58<3:32:12,  1.10s/it, lr=1e-5, step_loss=0.0404"
     ]
    }
   ],
   "source": [
    "MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "dataset_name=\"mohummadmahad/pakistani_fashion_dataset\"\n",
    "\n",
    "!accelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --caption_column=\"text\" \\\n",
    "  --image_column=\"image\" \\\n",
    "  --dataset_name=$dataset_name \\\n",
    "  --use_ema \\\n",
    "  --resolution=512 --center_crop --random_flip \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --gradient_checkpointing \\\n",
    "  --max_train_steps=15000 \\\n",
    "  --learning_rate=1e-05 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --output_dir=\"pakistani-fashion-generator-model\" \\\n",
    "  --push_to_hub \\\n",
    "  # --enable_xformers_memory_efficient_attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd6748d-b7ab-4f72-bc68-115a4f0575b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:41:12.970560Z",
     "iopub.status.busy": "2024-12-07T14:41:12.970134Z",
     "iopub.status.idle": "2024-12-07T14:41:12.988022Z",
     "shell.execute_reply": "2024-12-07T14:41:12.987343Z",
     "shell.execute_reply.started": "2024-12-07T14:41:12.970543Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from diffusers import StableDiffusionPipeline\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# import torch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# image = pipeline(prompt=\"a beautiful blue dress made for winter with gold embroidery\").images[0]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# image.save(\"dress.png\")\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMyModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msd-model-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmohummadmahad/pakistani_fashion_generator-model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyModel' is not defined"
     ]
    }
   ],
   "source": [
    "# from diffusers import StableDiffusionPipeline\n",
    "# import torch\n",
    "\n",
    "# pipeline = StableDiffusionPipeline.from_pretrained(\"sd-model-finetuned\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "# image = pipeline(prompt=\"a beautiful blue dress made for winter with gold embroidery\").images[0]\n",
    "# image.save(\"dress.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "933c8581-ddd3-453d-aeaf-bac31109e41c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:46:48.019954Z",
     "iopub.status.busy": "2024-12-07T14:46:48.019746Z",
     "iopub.status.idle": "2024-12-07T14:46:50.535371Z",
     "shell.execute_reply": "2024-12-07T14:46:50.534642Z",
     "shell.execute_reply.started": "2024-12-07T14:46:48.019938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2020.6.20)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a4adf3-77b0-4eb1-bfae-db6a311eb66c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T23:17:30.630167Z",
     "iopub.status.busy": "2024-12-07T23:17:30.629482Z",
     "iopub.status.idle": "2024-12-07T23:39:51.371038Z",
     "shell.execute_reply": "2024-12-07T23:39:51.369780Z",
     "shell.execute_reply.started": "2024-12-07T23:17:30.630146Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "401 Client Error. (Request ID: Root=1-6754dcc6-402b518354942e6576898dc6;f095af71-2e44-4084-a61c-055a832dea2c)\n\nRepository Not Found for url: https://huggingface.co/api/models/mohummadmahad/pakistani-fashion-generator-model/preupload/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.\nNote: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/mohummadmahad/pakistani-fashion-generator-model/preupload/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[1;32m      3\u001b[0m api \u001b[38;5;241m=\u001b[39m HfApi()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msd-model-finetuned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmohummadmahad/pakistani-fashion-generator-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUpload fine-tuned Stable Diffusion model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py:1208\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py:4598\u001b[0m, in \u001b[0;36mHfApi.upload_folder\u001b[0;34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, multi_commits, multi_commits_verbose, run_as_future)\u001b[0m\n\u001b[1;32m   4594\u001b[0m     \u001b[38;5;66;03m# Defining a CommitInfo object is not really relevant in this case\u001b[39;00m\n\u001b[1;32m   4595\u001b[0m     \u001b[38;5;66;03m# Let's return early with pr_url only (as string).\u001b[39;00m\n\u001b[1;32m   4596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pr_url\n\u001b[0;32m-> 4598\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4601\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4608\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4610\u001b[0m \u001b[38;5;66;03m# Create url to uploaded folder (for legacy return value)\u001b[39;00m\n\u001b[1;32m   4611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;129;01mand\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py:1208\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py:3558\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   3555\u001b[0m \u001b[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001b[39;00m\n\u001b[1;32m   3556\u001b[0m _warn_on_overwriting_operations(operations)\n\u001b[0;32m-> 3558\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3560\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munquoted_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# first-class methods take unquoted revision\u001b[39;49;00m\n\u001b[1;32m   3564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfree_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001b[39;49;00m\n\u001b[1;32m   3567\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3568\u001b[0m files_to_copy \u001b[38;5;241m=\u001b[39m _fetch_lfs_files_to_copy(\n\u001b[1;32m   3569\u001b[0m     copies\u001b[38;5;241m=\u001b[39mcopies,\n\u001b[1;32m   3570\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3574\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m   3575\u001b[0m )\n\u001b[1;32m   3576\u001b[0m commit_payload \u001b[38;5;241m=\u001b[39m _prepare_commit_payload(\n\u001b[1;32m   3577\u001b[0m     operations\u001b[38;5;241m=\u001b[39moperations,\n\u001b[1;32m   3578\u001b[0m     files_to_copy\u001b[38;5;241m=\u001b[39mfiles_to_copy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3581\u001b[0m     parent_commit\u001b[38;5;241m=\u001b[39mparent_commit,\n\u001b[1;32m   3582\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py:4027\u001b[0m, in \u001b[0;36mHfApi.preupload_lfs_files\u001b[0;34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[0m\n\u001b[1;32m   4025\u001b[0m \u001b[38;5;66;03m# Check which new files are LFS\u001b[39;00m\n\u001b[1;32m   4026\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4027\u001b[0m     \u001b[43m_fetch_upload_modes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_additions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgitignore_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgitignore_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4036\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4037\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   4038\u001b[0m     e\u001b[38;5;241m.\u001b[39mappend_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_commit_api.py:506\u001b[0m, in \u001b[0;36m_fetch_upload_modes\u001b[0;34m(additions, repo_type, repo_id, token, revision, endpoint, create_pr, gitignore_content)\u001b[0m\n\u001b[1;32m    498\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgitIgnore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m gitignore_content\n\u001b[1;32m    500\u001b[0m resp \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/preupload/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    502\u001b[0m     json\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    503\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    504\u001b[0m     params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_pr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    505\u001b[0m )\n\u001b[0;32m--> 506\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m preupload_info \u001b[38;5;241m=\u001b[39m _validate_preupload_info(resp\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    508\u001b[0m upload_modes\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]: file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muploadMode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m preupload_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_errors.py:323\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepoNotFound\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    305\u001b[0m     response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    326\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6754dcc6-402b518354942e6576898dc6;f095af71-2e44-4084-a61c-055a832dea2c)\n\nRepository Not Found for url: https://huggingface.co/api/models/mohummadmahad/pakistani-fashion-generator-model/preupload/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.\nNote: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"sd-model-finetuned\",\n",
    "    repo_id=\"mohummadmahad/pakistani-fashion-generator-model\",\n",
    "    commit_message=\"Upload fine-tuned Stable Diffusion model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0f4c3-6745-4b15-87e1-d947394354fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
